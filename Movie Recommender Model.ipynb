{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie Recommendation System - Complete Implementation\n",
    "\n",
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "# Cell 2: Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cell 3: Load and merge datasets\n",
    "print(\"Loading datasets...\")\n",
    "movies_raw = pd.read_csv('tmdb_5000_movies.csv')\n",
    "credits_raw = pd.read_csv('tmdb_5000_credits.csv')\n",
    "\n",
    "# Merge datasets on title\n",
    "movies = movies_raw.merge(credits_raw, on='title')\n",
    "movies = movies[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew', 'vote_average', 'vote_count']]\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "print(f\"Dataset loaded with {len(movies)} movies\")\n",
    "movies.head()\n",
    "\n",
    "# Cell 4: Calculate weighted ratings using IMDB formula\n",
    "print(\"Calculating weighted ratings...\")\n",
    "\n",
    "# Calculate the mean vote across all movies (C)\n",
    "C = movies['vote_average'].mean()\n",
    "print(f\"Mean vote average: {C:.2f}\")\n",
    "\n",
    "# Calculate the minimum number of votes required (90th percentile) (m)\n",
    "m = movies['vote_count'].quantile(0.9)\n",
    "print(f\"90th percentile vote count: {m}\")\n",
    "\n",
    "# Filter movies that meet the minimum vote count threshold\n",
    "q_movies = movies.copy().loc[movies['vote_count'] >= m]\n",
    "print(f\"Qualified movies: {len(q_movies)}\")\n",
    "\n",
    "# Function to calculate weighted rating based on IMDB formula\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "\n",
    "# Add the 'score' column to qualified movies\n",
    "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Sort movies by score in descending order\n",
    "q_movies = q_movies.sort_values('score', ascending=False)\n",
    "\n",
    "# Display top movies by weighted score\n",
    "print(\"Top 10 movies by weighted score:\")\n",
    "print(q_movies[['title', 'vote_average', 'vote_count', 'score']].head(10))\n",
    "\n",
    "# Use qualified movies for our recommendation model\n",
    "movies = q_movies.copy()\n",
    "\n",
    "# Cell 5: Define helper functions for data processing\n",
    "def convert(text):\n",
    "    \"\"\"Convert JSON-like strings to list of names\"\"\"\n",
    "    L = []\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            L.append(i['name'])\n",
    "    except:\n",
    "        pass\n",
    "    return L\n",
    "\n",
    "def convert_cast(text):\n",
    "    \"\"\"Get top 3 cast members\"\"\"\n",
    "    L = []\n",
    "    counter = 0\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            if counter < 3:\n",
    "                L.append(i['name'])\n",
    "                counter += 1\n",
    "    except:\n",
    "        pass\n",
    "    return L\n",
    "\n",
    "def fetch_director(text):\n",
    "    \"\"\"Fetch director name from crew\"\"\"\n",
    "    L = []\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            if i['job'] == 'Director':\n",
    "                L.append(i['name'])\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "    return L\n",
    "\n",
    "def collapse(L):\n",
    "    \"\"\"Remove spaces between names to create single tags\"\"\"\n",
    "    L1 = []\n",
    "    for i in L:\n",
    "        L1.append(i.replace(\" \", \"\"))\n",
    "    return L1\n",
    "\n",
    "# Cell 6: Define weighted processing functions\n",
    "def process_genres(obj):\n",
    "    \"\"\"Process and weight genres by repeating them\"\"\"\n",
    "    return collapse(convert(obj)) * 2  # Weight genres more\n",
    "\n",
    "def process_keywords(obj):\n",
    "    \"\"\"Process and weight keywords by repeating them\"\"\"\n",
    "    return collapse(convert(obj)) * 2  # Weight keywords more\n",
    "\n",
    "def process_director(obj):\n",
    "    \"\"\"Process and weight director by repeating them\"\"\"\n",
    "    return collapse(fetch_director(obj)) * 3  # Weight director most\n",
    "\n",
    "# Cell 7: Apply transformations with weighting\n",
    "print(\"Processing movie features...\")\n",
    "\n",
    "# Apply weighted processing to increase importance of key features\n",
    "movies['genres'] = movies['genres'].apply(process_genres)\n",
    "movies['keywords'] = movies['keywords'].apply(process_keywords)\n",
    "movies['cast'] = movies['cast'].apply(convert_cast).apply(collapse)\n",
    "movies['crew'] = movies['crew'].apply(process_director)  # This now contains weighted directors\n",
    "movies['overview'] = movies['overview'].apply(lambda x: x.split() if pd.notna(x) else [])\n",
    "\n",
    "# Display sample processed data\n",
    "print(\"Sample processed features:\")\n",
    "sample_movie = movies.iloc[0]\n",
    "print(f\"Title: {sample_movie['title']}\")\n",
    "print(f\"Genres: {sample_movie['genres'][:5]}...\")  # Show first 5 elements\n",
    "print(f\"Keywords: {sample_movie['keywords'][:5]}...\")\n",
    "print(f\"Cast: {sample_movie['cast']}\")\n",
    "print(f\"Director: {sample_movie['crew']}\")\n",
    "\n",
    "# Cell 8: Create tags and final dataframe\n",
    "print(\"Creating combined tags...\")\n",
    "\n",
    "# Combine all processed features into a single 'tags' column\n",
    "movies['tags'] = movies['overview'] + movies['genres'] + movies['keywords'] + movies['cast'] + movies['crew']\n",
    "\n",
    "# Create the final dataframe with essential columns\n",
    "new_df = movies[['movie_id', 'title', 'tags', 'score']].copy()\n",
    "\n",
    "# Convert the list of tags into a single, lowercase string\n",
    "new_df['tags'] = new_df['tags'].apply(lambda x: \" \".join(x).lower())\n",
    "\n",
    "print(f\"Final dataset shape: {new_df.shape}\")\n",
    "print(\"Sample tags:\")\n",
    "print(new_df.iloc[0]['tags'][:200] + \"...\")\n",
    "\n",
    "# Cell 9: Apply stemming\n",
    "print(\"Applying stemming to reduce word variations...\")\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    \"\"\"Apply stemming to text\"\"\"\n",
    "    y = []\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)\n",
    "\n",
    "# Apply stemming to tags\n",
    "new_df['tags'] = new_df['tags'].apply(stem)\n",
    "\n",
    "print(\"Sample stemmed tags:\")\n",
    "print(new_df.iloc[0]['tags'][:200] + \"...\")\n",
    "\n",
    "# Cell 10: Create TF-IDF vectors\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Transform tags into numerical vectors\n",
    "vectors = tfidf.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "print(f\"TF-IDF vector shape: {vectors.shape}\")\n",
    "print(f\"Feature names sample: {tfidf.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Computing cosine similarity matrix...\")\n",
    "similarity = cosine_similarity(vectors)\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity.shape}\")\n",
    "print(f\"Sample similarities for first movie: {similarity[0][:5]}\")\n",
    "\n",
    "# Cell 11: Fix index and test the recommendation function\n",
    "\n",
    "# FIX: Reset the index to ensure it is sequential from 0 to n-1\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "print(\"DataFrame index has been reset to prevent errors.\")\n",
    "\n",
    "def recommend(movie, movies_df, similarity_matrix):\n",
    "    \"\"\"Test function to get basic recommendations\"\"\"\n",
    "    try:\n",
    "        # Get the index (position) of the movie\n",
    "        index = movies_df[movies_df['title'] == movie].index[0]\n",
    "    except IndexError:\n",
    "        return \"Movie not found\"\n",
    "    \n",
    "    distances = similarity_matrix[index]\n",
    "    movies_list = sorted(list(enumerate(distances)), reverse=True, key=lambda x: x[1])\n",
    "    \n",
    "    recommendations = []\n",
    "    for i in movies_list[1:6]:  # Top 5, excluding the movie itself\n",
    "        movie_info = movies_df.iloc[i[0]]\n",
    "        recommendations.append({\n",
    "            'title': movie_info['title'],\n",
    "            'score': round(movie_info['score'], 2),\n",
    "            'similarity': round(i[1], 3)\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test with a popular movie\n",
    "test_movie = new_df.iloc[0]['title']\n",
    "print(f\"\\nTesting recommendations for: {test_movie}\")\n",
    "test_recommendations = recommend(test_movie, new_df, similarity)\n",
    "for i, rec in enumerate(test_recommendations, 1):\n",
    "    print(f\"{i}. {rec['title']} (Score: {rec['score']}, Similarity: {rec['similarity']})\")\n",
    "\n",
    "# Cell 12: Save the model and data\n",
    "print(\"\\nSaving model files...\")\n",
    "\n",
    "# Save the processed dataframe and similarity matrix\n",
    "pickle.dump(new_df.to_dict(), open('movies_list_enhanced.pkl', 'wb'))\n",
    "pickle.dump(similarity, open('similarity_enhanced.pkl', 'wb'))\n",
    "\n",
    "print(\"Model and data saved successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"- movies_list_enhanced.pkl\")\n",
    "print(\"- similarity_enhanced.pkl\")\n",
    "\n",
    "# Cell 13: Verify saved files\n",
    "print(\"\\nVerifying saved files...\")\n",
    "\n",
    "# Load and verify the saved files\n",
    "try:\n",
    "    loaded_movies = pickle.load(open('movies_list_enhanced.pkl', 'rb'))\n",
    "    loaded_similarity = pickle.load(open('similarity_enhanced.pkl', 'rb'))\n",
    "    \n",
    "    loaded_df = pd.DataFrame(loaded_movies)\n",
    "    \n",
    "    print(f\"Loaded movies shape: {loaded_df.shape}\")\n",
    "    print(f\"Loaded similarity shape: {loaded_similarity.shape}\")\n",
    "    print(\"✅ Files loaded successfully!\")\n",
    "    \n",
    "    # Test one more recommendation with loaded data\n",
    "    print(f\"\\nFinal test with loaded data for: {test_movie}\")\n",
    "    final_test = recommend(test_movie, loaded_df, loaded_similarity)\n",
    "    for i, rec in enumerate(final_test[:3], 1):\n",
    "        print(f\"{i}. {rec['title']} (Score: {rec['score']})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading files: {e}\")\n",
    "\n",
    "print(\"\\n🎉 Movie Recommendation System setup complete!\")\n",
    "print(\"You can now run the Streamlit app with: streamlit run app.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
